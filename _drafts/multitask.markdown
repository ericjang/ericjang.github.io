
% Machine Learning capabilities have advanced considerably in recent years, largely buoyed by the increasing availability of compute \citep{sevilla2022compute}, large-scale datasets crawled from the Internet \todo{cite}, and improved algorithms for learning \citep{vaswani2017attention}. \cite{kaplan2020scaling} demonstrated an empirical power law relationship between the capacity of a language model (NLP terminology for a next-token autoregressive generative model) and its performance (negative log likelihood on held-out data). As performance on this relatively straightforward task increases, performance on other benchmarks - question answering, few-shot learning, alignment, toxicity, etc. tend to improve as well, consistent with \cite{mahoney2006rationale}'s claims that lossless compression of human-written text must recover general capabilities \todo{this may be a strong claim, maybe we should remove it}. The empirical consistency of scaling laws have motivated considerable investment into building much larger language models. \KH{I feel uncomfortable that we are making such a big claim about scaling (also a very vague claim) at the beginning of intro without showing multi-task model is better than single-task model (scaling could have many aspects: data, model size, number of tasks). I think a better claim would be (1) stressing on that this works better than anything else for the multi-task setting, and (2) ``within the multi-task setting``, the performance scales with model size}

% It's natural to wonder whether there are analogous ``scaling laws'' that occur in other machine\todo{this already happened in other domains like vision} learning domains, such as learning to solve difficult reinforcement learning environments. 

Concurrent works on Decision Transformer  \cite{chen2021decision} and Trajectory Transformer \cite{janner2021offline} have shown impressive results on policies trained from purely offline data, using essentially the same model architecture employed in GPT-2 \todo{cite}. We ask the question: ``does increased model scaling improve generalization in RL environments? Are there parallels between RL and NLP in the transfer to out-of-domain tasks when one trains on large datasets with increasingly large models?''

We study this by investigating whether scaling up model capacity on a ``scale-friendly'' \KH{not clear} class of model architectures (such as Decision Transformers) enables breakthroughs in offline reinforcement learning for RL. We extend Decision Transformers to condition the policy on predicted model-free returns, investigating their scaling properties empirically. We find that at a sufficient scale of data and model size, it is possible to train using only offline conditional supervised learning to train single neural network capable of playing all 50 Atari games. 

To date, the majority of algorithms developed for learning to solve the Atari-57 suite solves each game separately. IMPALA \cite{espeholt2018impala} was the first to train on all games with a single set of weights, although the human-normalized-return was substantially lower than single-game performance.

Why is solving multiple Atari games a relevant benchmark, compared to the traditional method of using a single algorithm to solve each game separately? Existing RL algorithms, both online and offline (but especially online) struggle to learn a good policy that can work well across all games, hinting that present RL algorithms have some scalability and generalization challenges in multi-task settings. 

% Multi-task RL vs. single-task RL is like the question of fighting 100 duck-sized horses or 1 horse-sized duck.

Playing multiple atari games isn't the end goal in and of itself; rather, studying the scalability of offline algorithms to successively harder problems. Therefore, a model that can play all games simultaneously is really a study of how well our existing RL algorithms scale to the next era of RL benchmarks. 