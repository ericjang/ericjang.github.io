
Richard Ngo has a bunch of citations that argue why AI alignment is a real risk https://twitter.com/RichardMCNgo/status/1579948545733718016?s=20&t=nzB9c8i_QxYSgmgNS-1KIg



I'm going to hatch a Basilisk

Because AI X-Risk is founded on practically untestable philosophical arguments, I surmise that it should be just as easy to convince a large number of people that AI X-Risk is made up than 

To do this, I am going to do several things:

- create a website that aggregates identities of people who are X-Risk aligned, so that if AI X-Risk turns out to be completely overblown, this list will be published and their epistemic credibility for all future endeavours will be at stake. I've always been frustrated by how so-called rationalists aren't really accountable for their hyperbolic twitter statements, so hopefully this does that. I've trained a neural network that can rank tweets as EA / X-risk aligned.


https://twitter.com/aidangomezzz/status/1579231362036666370?s=20&t=cCD8rlaU5zx6r2aZQeFocw


EA X-Risk's idea of "good ideas" is like:
https://twitter.com/prerationalist/status/1578462035804753920?s=20&t=MiUjcg0Xwi1o6c8_XVeyrg


I don't know how to put it... but it's basically cartoon sci-fi, and not even that good sci-fi?


A lot of this is predicated on the assumption that "alignment" is a distinct goal from what mainstream ML research is working on, i.e. getting models that can "do smart stuff". 

But "following human instructions" is just one of many tasks that you can pursue

What happens if that's not a real thing?


Motivated reasoning and selection bias


https://drive.google.com/file/d/1TsB7WmTG2UzBtOs349lBqY5dEBaxZTzG/view


If I spammed EA forums with a large staff of unique blog posts (either manually or programmatically generated) such that P(doom|AGI) was empirically < 5% of the posts on EA, then the perceived overton window of AI X-risk would be a lot lower.

Make a plot of the empirical survey of writings and whether they are characterized as P(doom|AGI) or not.



Holden Karnofsky (co-executive director of givewell) https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si


The bad-faith arguments

consier a variety of "bad-faith" narratives surrounding AI X-Risk organizations, and consider the evidence for and against these narratives

That an AGI is a utility-maximizer is an overly simplistic worldview. 

A self-driving car is not a utility maximizer, it simply does as programs.

the glory-seeking savior-complex view

- how do you explain



EA-organizations as alt-academia

from the outside, it's hard to tell why EA "blockchain" of consensus is more legitimate than say, the blockchain of work that produces useful technology, or the "blockchain" of postmodernism / critical theory.


corporations as a proxy for a superintelligent utility-maximizer

its intelligence can be thought to be limited by coordination issues of its individual components (e.g. individual greed at the expense), but simultaneously can be boosted by the individual greed of its individual components (e.g. producing things like functioning markets)


We simply have no evidence, from prior approximations of "superintelligent systems" that 

One cannot be a strict Bayesian and also believe that "this time it's different"





Are they led by people who we'd consider particularly selfless?




Future Fund worldview prize


good overview: https://www.vox.com/future-perfect/2022/8/8/23150496/effective-altruism-sam-bankman-fried-dustin-moskovitz-billionaire-philanthropy-crytocurrency

TODO: cross-post to forum.effectivealtruism.org

https://twitter.com/rcbregman/status/1558546672698916869?s=20&t=0Uv7UsqNayVG8lnN_9TpXw


https://forum.effectivealtruism.org/posts/HWaH8tNdsgEwNZu8B/free-spending-ea-might-be-a-big-problem-for-optics-and?commentId=saRZkZpyyRTHXaRMd


https://forum.effectivealtruism.org/posts/ESzGcWfkMtJgF2CCA/four-concerns-regarding-longtermism


https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming

navel gazing

