
- data augmentation
- weight decay (needed for grokking paper)
- training with VIB. Bayesian uncertainty (alemi has a point on how the bayesian interpretation of MLE on infinite epochs of training as being equivalent to seeing the same evidence an infinite number of times, whereas you want to treat the evidence as finite from a Bayes rule perspective.) 